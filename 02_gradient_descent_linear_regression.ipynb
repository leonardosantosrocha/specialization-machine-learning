{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descida do Gradiente na Regressão Linear\n",
    "\n",
    "- O que é ?\n",
    "\n",
    "    - É um algoritmo de otimização que tem por objetivo encontrar os pesos (ou valores) do modelo que irão minimizar a função de custo (ou erro). Onde a função de custo representa o quão bem o modelo se ajustou em relação aos dados.\n",
    "\n",
    "- Como funciona o algoritmo ?\n",
    "\n",
    "    - Iniciamos com uma suposição inicial para os valores dos parâmetros do modelo - por convenção adota-se 0.\n",
    "\n",
    "    - Localizamos a direção do gradiente, e em seguida, seguimos a direção oposta, visto que, ao seguir a direção do gradiente estaríamos aumentando o valor da função de custo (ou erro) - o que não é nosso objetivo.\n",
    "\n",
    "    - Enquanto ponto atual diferente do ponto mínimo global: \n",
    "        \n",
    "        - Realizamos o [cálculo do gradiente](https://pt.wikipedia.org/wiki/M%C3%A9todo_do_gradiente) que considera a taxa de aprendizado - quanto maior a taxa de aprendizado, mais longo será o \"passo\" entre uma iteração e outra, por isso, aconselha-se iniciar com um passo maior e reduzí-lo ao longo das iterações.\n",
    "\n",
    "- Quais são as variações do algoritmo ?\n",
    "\n",
    "    - Descida do Gradiente Estocástico: atualiza os parâmetros usando um único exemplo de treinamento por iteração, o que pode acelerar o processo de treinamento.\n",
    "\n",
    "    - Descida do Gradiente em Batch: usa uma amostra de dados para calcular o gradiente, o que gera equilíbrio entre eficiência e precisão.\n",
    "    \n",
    "<br/>\n",
    "\n",
    "<img src=\"_img/03.png\">"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
